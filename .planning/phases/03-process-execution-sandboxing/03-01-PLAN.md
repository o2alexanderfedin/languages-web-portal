---
phase: 03-process-execution-sandboxing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/shared/src/types/execution.ts
  - packages/shared/src/types/index.ts
  - packages/server/src/config/limits.ts
  - packages/server/src/config/toolRegistry.ts
  - packages/server/src/services/queueService.ts
  - packages/server/src/middleware/rateLimiter.ts
  - packages/server/package.json
  - packages/server/src/__tests__/queueService.test.ts
  - packages/server/src/__tests__/rateLimiter.test.ts
autonomous: true

must_haves:
  truths:
    - "Server limits concurrent executions to CPU core count and queues excess requests"
    - "Server provides queue position and estimated wait time when at capacity"
    - "Server implements per-IP rate limiting (5 concurrent jobs, 20/hour)"
    - "Server gracefully degrades under load (queues requests instead of crashing)"
  artifacts:
    - path: "packages/shared/src/types/execution.ts"
      provides: "Execution type contracts shared between client and server"
      exports: ["ExecutionRequest", "ExecutionResponse", "QueueStatus", "JobStatus", "ToolExecutionConfig"]
    - path: "packages/server/src/config/limits.ts"
      provides: "Resource limit constants for execution"
      exports: ["EXECUTION_LIMITS", "RATE_LIMITS", "QUEUE_CONFIG"]
    - path: "packages/server/src/config/toolRegistry.ts"
      provides: "Tool CLI definitions with command paths, args, status"
      exports: ["getToolConfig", "getAvailableTools", "ToolExecutionConfig"]
    - path: "packages/server/src/services/queueService.ts"
      provides: "Concurrency control with p-queue"
      exports: ["QueueService", "queueService"]
      min_lines: 60
    - path: "packages/server/src/middleware/rateLimiter.ts"
      provides: "Per-IP rate limiting middleware"
      exports: ["hourlyRateLimit", "concurrentExecutionLimit"]
      min_lines: 40
  key_links:
    - from: "packages/server/src/services/queueService.ts"
      to: "p-queue"
      via: "PQueue concurrency control"
      pattern: "new PQueue"
    - from: "packages/server/src/config/toolRegistry.ts"
      to: "packages/shared/src/constants/tools.ts"
      via: "TOOLS array import for metadata"
      pattern: "import.*TOOLS.*from.*@repo/shared"
    - from: "packages/server/src/middleware/rateLimiter.ts"
      to: "packages/server/src/config/limits.ts"
      via: "Rate limit constants"
      pattern: "RATE_LIMITS"
---

<objective>
Build the foundational execution infrastructure: shared types, resource limit config, tool registry, concurrency queue service, and per-IP rate limiter middleware.

Purpose: These are the building blocks that Plan 02 (execution service + route) and Plan 03 (client tool picker) depend on. Types define the API contract. The queue service enforces concurrency limits. The rate limiter prevents abuse. All must exist before the execute endpoint can be wired up.

Output: Shared execution types importable by both client and server. Queue service that limits concurrent executions to CPU core count with position tracking. Rate limiter middleware enforcing 5 concurrent/20 per hour per IP. Tool registry mapping all 8 tools to CLI execution configs.
</objective>

<execution_context>
@/Users/alexanderfedin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/alexanderfedin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-process-execution-sandboxing/03-RESEARCH.md

@packages/shared/src/types/tool.ts
@packages/shared/src/types/api.ts
@packages/shared/src/types/upload.ts
@packages/shared/src/types/index.ts
@packages/shared/src/constants/tools.ts
@packages/shared/src/index.ts
@packages/server/src/config/env.ts
@packages/server/src/types/errors.ts
@packages/server/package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Shared execution types, resource limits config, and tool registry</name>
  <files>
    packages/shared/src/types/execution.ts
    packages/shared/src/types/index.ts
    packages/server/src/config/limits.ts
    packages/server/src/config/toolRegistry.ts
  </files>
  <action>
    **1. Create shared execution types** (`packages/shared/src/types/execution.ts`):
    - `JobStatus` type: `'queued' | 'running' | 'completed' | 'failed' | 'timeout' | 'cancelled'`
    - `ExecutionRequest` interface: `{ toolId: string; projectId: string }` -- what the client sends to start execution
    - `ExecutionResponse` interface: `{ jobId: string; status: JobStatus; exitCode?: number; output?: string[]; error?: string; startedAt?: string; completedAt?: string; durationMs?: number }` -- what the server returns after execution
    - `QueueStatus` interface: `{ position: number; pending: number; concurrency: number; estimatedWaitMs: number; estimatedWaitSec: number }` -- queue information for client display
    - `ToolExecutionConfig` interface: `{ id: string; command: string; defaultArgs: string[]; maxExecutionTimeMs: number; available: boolean }` -- server-side CLI config (shared so client can reference the type if needed)
    - Export all from `packages/shared/src/types/index.ts` barrel: add `export * from './execution.js';`

    **2. Create resource limits config** (`packages/server/src/config/limits.ts`):
    - Import `os` for CPU count
    - `EXECUTION_LIMITS` as const: `{ maxTimeoutMs: 60_000, defaultTimeoutMs: 30_000, maxOutputLines: 10_000 }`
    - `RATE_LIMITS` as const: `{ maxConcurrentPerIp: 5, maxPerHour: 20, windowMs: 60 * 60 * 1000 }`
    - `QUEUE_CONFIG` as const: `{ concurrency: os.cpus().length, defaultEstimateMs: 30_000, maxDurationHistory: 100 }`
    - All objects exported individually with const assertion for maximum type safety

    **3. Create tool registry** (`packages/server/src/config/toolRegistry.ts`):
    - Import `TOOLS` from `@repo/shared` for tool metadata (id, name, status)
    - Import `ToolExecutionConfig` from `@repo/shared`
    - Create `TOOL_EXECUTION_CONFIGS: Map<string, ToolExecutionConfig>` populated from a config array
    - For each of the 8 tools in TOOLS, create a ToolExecutionConfig entry:
      - `cpp-to-c-transpiler`: `{ command: '/usr/local/bin/hapyy-cpp2c', defaultArgs: ['--input'], maxExecutionTimeMs: 60000, available: true }`
      - `cpp-to-rust-transpiler`: `{ command: '/usr/local/bin/hapyy-cpp2rust', defaultArgs: ['--input'], maxExecutionTimeMs: 60000, available: true }`
      - All other 6 tools: `available: false` with placeholder commands
    - `getToolConfig(toolId: string): ToolExecutionConfig | undefined` -- Map.get wrapper
    - `getAvailableTools(): ToolExecutionConfig[]` -- filters to available=true entries
    - Note: command paths are placeholders. Tools aren't installed yet. This is expected -- the execution engine is ready, actual binaries are configured during deployment.
  </action>
  <verify>
    - `npm run build` passes with no TypeScript errors across all packages
    - `npx eslint packages/server/src/config/ packages/shared/src/types/` has no errors
    - Verify execution.ts types are importable: `import { ExecutionRequest, QueueStatus } from '@repo/shared'` compiles
    - Verify TOOLS.length === 8 tools in tool registry, 2 marked available
  </verify>
  <done>
    - Shared execution types (ExecutionRequest, ExecutionResponse, QueueStatus, JobStatus, ToolExecutionConfig) exported from @repo/shared
    - Resource limits config defines timeout (60s), rate limits (5 concurrent, 20/hour), queue concurrency (CPU count)
    - Tool registry has all 8 tools with CLI execution configs, 2 available, 6 unavailable
    - TypeScript build succeeds, ESLint clean
  </done>
</task>

<task type="auto">
  <name>Task 2: Queue service with p-queue and rate limiter middleware with tests</name>
  <files>
    packages/server/src/services/queueService.ts
    packages/server/src/middleware/rateLimiter.ts
    packages/server/package.json
    packages/server/src/__tests__/queueService.test.ts
    packages/server/src/__tests__/rateLimiter.test.ts
  </files>
  <action>
    **1. Install dependencies:**
    - Run `npm install p-queue express-rate-limit -w @repo/server`
    - p-queue v8 is ESM-only -- compatible with our ESM project (type: "module" in package.json)
    - express-rate-limit v7 works with Express 4

    **2. Create queue service** (`packages/server/src/services/queueService.ts`):
    - Import PQueue from `p-queue`
    - Import `QUEUE_CONFIG` from `../config/limits.js`
    - Import `QueueStatus` from `@repo/shared`
    - Class `QueueService`:
      - Private `queue: PQueue` initialized with `{ concurrency: QUEUE_CONFIG.concurrency, autoStart: true }`
      - Private `jobDurations: number[]` -- rolling window of last N job durations for wait estimation
      - `async addJob<T>(fn: () => Promise<T>): Promise<T>` -- adds function to queue, tracks start/end time, pushes duration to jobDurations (trim to maxDurationHistory), returns result via `this.queue.add()`
      - `getQueueStatus(): QueueStatus` -- returns `{ position: this.queue.size, pending: this.queue.pending, concurrency: this.queue.concurrency, estimatedWaitMs, estimatedWaitSec: Math.ceil(estimatedWaitMs / 1000) }`
      - Private `getAverageDuration(): number` -- returns mean of jobDurations, or QUEUE_CONFIG.defaultEstimateMs if empty
      - Estimated wait = `Math.ceil(position / concurrency) * avgDuration`
    - Export singleton `queueService = new QueueService()`
    - Follow research Pattern 2 (Concurrency Control with p-queue)

    **3. Create rate limiter middleware** (`packages/server/src/middleware/rateLimiter.ts`):
    - Import rateLimit from `express-rate-limit`
    - Import `RATE_LIMITS` from `../config/limits.js`
    - Import types: `Request, Response, NextFunction` from `express`
    - `hourlyRateLimit`: `rateLimit({ windowMs: RATE_LIMITS.windowMs, max: RATE_LIMITS.maxPerHour, standardHeaders: true, legacyHeaders: false, keyGenerator: (req) => req.ip || 'unknown', skip: (req) => req.path === '/health', handler: (req, res) => { res.status(429).json({ error: 'Rate limit exceeded', limit: RATE_LIMITS.maxPerHour, windowMs: RATE_LIMITS.windowMs }); } })`
    - `concurrentExecutionLimit`: Custom middleware using `Map<string, number>` to track active executions per IP
      - On request: get IP, check count >= RATE_LIMITS.maxConcurrentPerIp -> 429 with `{ error: 'Too many concurrent executions', limit: RATE_LIMITS.maxConcurrentPerIp, current }`
      - If allowed: increment count, register `res.on('finish', ...)` to decrement (delete entry when count reaches 0 to prevent memory leak)
      - Call `next()` to proceed
    - Follow research Pattern 3 (Per-IP Rate Limiting)

    **4. Write queue service tests** (`packages/server/src/__tests__/queueService.test.ts`):
    - Use vitest, follow existing test file patterns
    - Test: addJob executes function and returns result
    - Test: jobs run concurrently up to concurrency limit (create N+1 jobs, verify N start immediately, 1 is queued)
    - Test: getQueueStatus returns correct position when jobs are queued
    - Test: getQueueStatus returns estimatedWaitMs based on job history
    - Test: getQueueStatus returns default estimate when no history exists
    - Test: job duration tracking works (add job, verify internal duration tracked)
    - Minimum 5 tests

    **5. Write rate limiter tests** (`packages/server/src/__tests__/rateLimiter.test.ts`):
    - Use vitest + supertest, create minimal Express app for testing
    - Test: requests within limit return 200
    - Test: requests exceeding hourly limit return 429 with correct error body
    - Test: concurrent limit tracks active requests (mock a slow handler, verify limit enforced)
    - Test: concurrent limit decrements count after response completes
    - Test: concurrent limit returns 429 when exceeded with correct body
    - Minimum 5 tests
  </action>
  <verify>
    - `npm run build` passes with no TypeScript errors
    - `npm test -w @repo/server` passes all existing tests + new queueService and rateLimiter tests
    - `npx eslint packages/server/src/ packages/shared/src/` has no errors
    - queueService.test.ts has >= 5 tests
    - rateLimiter.test.ts has >= 5 tests
    - `cat packages/server/package.json | grep p-queue` shows p-queue in dependencies
    - `cat packages/server/package.json | grep express-rate-limit` shows express-rate-limit in dependencies
  </verify>
  <done>
    - Queue service limits concurrency to CPU core count, provides queue status with position and estimated wait time
    - Rate limiter enforces 20 requests/hour per IP and 5 concurrent executions per IP
    - Both return proper 429 responses when limits exceeded
    - p-queue and express-rate-limit installed as server dependencies
    - All tests pass (existing + 10+ new), build succeeds, ESLint clean
  </done>
</task>

</tasks>

<verification>
1. `npm run build` -- full project compiles without errors
2. `npm test -w @repo/server` -- all tests pass (existing + new queue/rate limiter tests)
3. `npx eslint packages/server/src/ packages/shared/src/` -- zero errors
4. Shared types importable from @repo/shared in both client and server
5. Queue service singleton exported and functional
6. Rate limiter middleware exported and functional
7. Tool registry has 8 entries, 2 available
</verification>

<success_criteria>
- Shared execution types define the API contract between client and server (ExecutionRequest, ExecutionResponse, QueueStatus, JobStatus)
- Queue service limits concurrent jobs to os.cpus().length and tracks position + estimated wait
- Rate limiter blocks IPs exceeding 20/hour or 5 concurrent with 429 responses
- Tool registry maps all 8 Hapyy tools to CLI execution configs with availability flags
- All tests pass (expect 60+ total server tests), build succeeds, ESLint clean
</success_criteria>

<output>
After completion, create `.planning/phases/03-process-execution-sandboxing/03-01-SUMMARY.md`
</output>
